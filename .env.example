# ============================================================================
# Reachy Mini Conversation App - Fully Local Configuration
# ============================================================================
# This app runs completely locally with no cloud dependencies.
# Optimized for edge devices like Jetson Nano Super 8GB.
# ============================================================================

# ============================================================================
# FULL LOCAL MODE (Always enabled - no cloud dependencies)
# ============================================================================
FULL_LOCAL_MODE=true
JETSON_OPTIMIZE=false

# ============================================================================
# LOCAL LLM CONFIGURATION (Required)
# ============================================================================
# Select which local LLM provider to use: "lmstudio" or "ollama"
LLM_PROVIDER=ollama

# LM Studio configuration (when LLM_PROVIDER=lmstudio)
LMSTUDIO_ENDPOINT=http://localhost:1234/v1
LMSTUDIO_MODEL=Phi-3-mini-4k-instruct.Q4_K_M.gguf

# Ollama configuration (when LLM_PROVIDER=ollama) - Recommended for Jetson
OLLAMA_ENDPOINT=http://localhost:11434/v1
# Recommended models:
# - gemma3:1b: 1B params, ~1GB RAM (fast, less accurate)
# - gemma3:4b: 4B params, ~4GB RAM (slower, more accurate)
OLLAMA_MODEL=gemma3:1b

# ============================================================================
# LOCAL ASR CONFIGURATION (Speech-to-Text - Distil-Whisper)
# ============================================================================
# Distil-Whisper model selection
# Options:
# - distil-whisper/distil-small.en: Fastest, English-only (recommended for Jetson)
# - distil-whisper/distil-medium.en: Better accuracy, slower
# - distil-whisper/distil-large-v3: Best quality, heaviest
DISTIL_WHISPER_MODEL=distil-whisper/distil-small.en
WHISPER_LANGUAGE=en

# ============================================================================
# LOCAL TTS CONFIGURATION (Text-to-Speech - Kokoro via FastRTC)
# ============================================================================
# Kokoro (lightweight, 82M parameters, via FastRTC built-in support)

# Voice selection:
# - af_sarah: American Female (Sarah)
# - am_michael: American Male (Michael)
# - bf_emma: British Female (Emma)
# - bm_lewis: British Male (Lewis)
KOKORO_VOICE=af_sarah

# Speech speed multiplier (0.5-2.0, default 1.0)
KOKORO_SPEED=1.0

# ============================================================================
# LOCAL VAD CONFIGURATION (Voice Activity Detection)
# ============================================================================
# Built-in energy-based VAD settings
VAD_ENERGY_THRESHOLD=0.05
VAD_SILENCE_DURATION=0.8
VAD_MIN_SPEECH_DURATION=0.3

# ============================================================================
# ONNX RUNTIME OPTIMIZATION (for Jetson CUDA acceleration)
# ============================================================================
ONNX_PROVIDERS=CUDAExecutionProvider,CPUExecutionProvider

# ============================================================================
# VISION CONFIGURATION (Optional - for camera tools)
# ============================================================================
# Local vision model (only used with --local-vision CLI flag)
LOCAL_VISION_MODEL=HuggingFaceTB/SmolVLM2-2.2B-Instruct

# Cache directory for Hugging Face models
# Tip: Use external storage if available (e.g., /mnt/ssd/hf_cache)
HF_HOME=./cache

# Hugging Face token (optional, only for gated models)
HF_TOKEN=

# ============================================================================
# CUSTOM PROFILE (Optional - personality customization)
# ============================================================================
# To select a specific profile with custom instructions and tools
# Place in profiles/<myprofile>/__init__.py
# REACHY_MINI_CUSTOM_PROFILE=friendly_assistant

# ============================================================================
# NOTES FOR DEPLOYMENT
# ============================================================================
# 1. For Jetson Nano: Use the .env.jetson template for optimized settings
# 2. Install Ollama: curl -fsSL https://ollama.com/install.sh | sh
# 3. Pull model: ollama pull gemma3:1b
# 4. Install app: pip install -e "."
# 5. Run: reachy-mini-conversation-app
#
# Expected performance on Jetson Nano Super 8GB:
# - End-to-end latency: <3 seconds
# - Peak memory usage: ~3GB (leaves 5GB free)
# - Continuous operation: Stable for hours
